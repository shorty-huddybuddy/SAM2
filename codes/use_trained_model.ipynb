{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9758a22b-4f85-424d-b33d-96cd1dc75715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# segment image region using  fine tune model\n",
    "# See Train.py on how to fine tune/train the model\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "\n",
    "# use bfloat16 for the entire script (memory efficient)\n",
    "torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16).__enter__()\n",
    "\n",
    "# Load image\n",
    "\n",
    "image_path = r\"C:\\Users\\sinha\\Desktop\\python\\captured_image.png\" # path to image\n",
    "mask_path = r\"C:\\Users\\sinha\\segment-anything-2\\notebooks\\mask.png\" # path to mask, the mask will define the image region to segment\n",
    "\n",
    "def read_image(image_path, mask_path): # read and resize image and mask\n",
    "        img = cv2.imread(image_path)[...,::-1]  # read image as rgb\n",
    "        mask = cv2.imread(mask_path,0) # mask of the region we want to segment\n",
    "\n",
    "        # Resize image to maximum size of 1024\n",
    "\n",
    "        r = np.min([1024 / img.shape[1], 1024 / img.shape[0]])\n",
    "        img = cv2.resize(img, (int(img.shape[1] * r), int(img.shape[0] * r)))\n",
    "        mask = cv2.resize(mask, (int(mask.shape[1] * r), int(mask.shape[0] * r)),interpolation=cv2.INTER_NEAREST)\n",
    "        return img, mask\n",
    "image,mask = read_image(image_path, mask_path)\n",
    "num_samples = 30 # number of points/segment to sample\n",
    "def get_points(mask,num_points): # Sample points inside the input mask\n",
    "        points=[]\n",
    "        for i in range(num_points):\n",
    "            coords = np.argwhere(mask > 0)\n",
    "            yx = np.array(coords[np.random.randint(len(coords))])\n",
    "            points.append([[yx[1], yx[0]]])\n",
    "        return np.array(points)\n",
    "input_points = get_points(mask,num_samples)\n",
    "# read image and sample points\n",
    "\n",
    "\n",
    "# Load model you need to have pretrained model already made\n",
    "sam2_checkpoint = r\"C:\\Users\\sinha\\segment-anything-2\\checkpoints\\sam2_hiera_small.pt\" # \"sam2_hiera_large.pt\"\n",
    "model_cfg = \"sam2_hiera_s.yaml\" # \"sam2_hiera_l.yaml\"\n",
    "sam2_model = build_sam2(model_cfg, sam2_checkpoint, device=\"cuda\")\n",
    "\n",
    "# Build net and load weights\n",
    "predictor = SAM2ImagePredictor(sam2_model)\n",
    "predictor.model.load_state_dict(torch.load(\"model.torch\"))\n",
    "\n",
    "# predict mask\n",
    "\n",
    "with torch.no_grad():\n",
    "        predictor.set_image(image)\n",
    "        masks, scores, logits = predictor.predict(\n",
    "            point_coords=input_points,\n",
    "            point_labels=np.ones([input_points.shape[0],1])\n",
    "        )\n",
    "\n",
    "# Short predicted masks from high to low score\n",
    "\n",
    "masks=masks[:,0].astype(bool)\n",
    "shorted_masks = masks[np.argsort(scores[:,0])][::-1].astype(bool)\n",
    "\n",
    "# Stitch predicted mask into one segmentation mask\n",
    "\n",
    "seg_map = np.zeros_like(shorted_masks[0],dtype=np.uint8)\n",
    "occupancy_mask = np.zeros_like(shorted_masks[0],dtype=bool)\n",
    "for i in range(shorted_masks.shape[0]):\n",
    "    mask = shorted_masks[i]\n",
    "    if (mask*occupancy_mask).sum()/mask.sum()>0.15: continue\n",
    "    mask[occupancy_mask]=0\n",
    "    seg_map[mask]=i+1\n",
    "    occupancy_mask[mask]=1\n",
    "\n",
    "# create colored annotation map\n",
    "height, width = seg_map.shape\n",
    "\n",
    "# Create an empty RGB image for the colored annotation\n",
    "rgb_image = np.zeros((seg_map.shape[0], seg_map.shape[1], 3), dtype=np.uint8)\n",
    "for id_class in range(1,seg_map.max()+1):\n",
    "    rgb_image[seg_map == id_class] = [np.random.randint(255), np.random.randint(255), np.random.randint(255)]\n",
    "\n",
    "# save and display\n",
    "\n",
    "cv2.imwrite(\"annotation.png\",rgb_image)\n",
    "# cv2.imwrite(\"mix.png\",(rgb_image/2+image/2).astype(np.uint8))\n",
    "cv2.imwrite(\"mix.png\",(2 * (rgb_image/5)+3*(image/5)).astype(np.uint8))\n",
    "cv2.imwrite(\"image.png\" , (image))\n",
    "\n",
    "cv2.imshow(\"annotation\",rgb_image)\n",
    "# cv2.imshow(\"mix\",(rgb_image / 2 + image / 2).astype(np.uint8))\n",
    "cv2.imshow(\"mix\",(2 * (rgb_image / 5) + 3 * (image / 5)).astype(np.uint8))\n",
    "cv2.imshow(\"image\",image)\n",
    "cv2.waitKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb74bf27-b680-4f25-a85a-d79a29cd5e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. ... 1. 1. 1.]\n",
      "  [0. 0. 0. ... 1. 1. 1.]\n",
      "  [0. 0. 0. ... 1. 1. 1.]\n",
      "  ...\n",
      "  [0. 0. 1. ... 1. 1. 1.]\n",
      "  [0. 0. 0. ... 0. 1. 1.]\n",
      "  [0. 0. 0. ... 0. 1. 1.]]]\n"
     ]
    }
   ],
   "source": [
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "from sam2.build_sam import build_sam2\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Define paths\n",
    "checkpoint_path = r\"C:\\Users\\sinha\\segment-anything-2\\notebooks\\model.torch\"\n",
    "config_file = \"sam2_hiera_l.yaml\"  # Adjust this path as needed\n",
    "\n",
    "# Initialize SAM model and predictor\n",
    "sam_model = build_sam2(checkpoint_path=checkpoint_path, config_file=config_file)\n",
    "predictor = SAM2ImagePredictor(sam_model)\n",
    "\n",
    "mask_path = r\"C:\\Users\\sinha\\segment-anything-2\\notebooks\\mask.png\"  # Adjust the mask path if needed\n",
    "\n",
    "# Load and preprocess image\n",
    "image_path = r\"C:\\Users\\sinha\\Desktop\\python\\captured_image.png\"# Add the path to your image\n",
    "image = cv2.imread(image_path)[..., ::-1]  # BGR to RGB\n",
    "\n",
    "# Create a copy of the image array to avoid negative strides issue\n",
    "image = image.copy()\n",
    "\n",
    "predictor.set_image(image)\n",
    "\n",
    "# Define input points and labels for segmentation\n",
    "input_point = np.array([[587, 447]])  # replace x, y with coordinates\n",
    "input_label = np.array([1])  # 1 for foreground, 0 for background\n",
    "\n",
    "# Generate mask\n",
    "# mask_input, unnorm_coords, labels, unnorm_box = predictor._prep_prompts(input_point, input_label)\n",
    "# sparse_embeddings, dense_embeddings = predictor.model.sam_prompt_encoder(\n",
    "#     points=(unnorm_coords, labels), boxes=None, masks=None,\n",
    "# )\n",
    "# prd_masks, prd_scores, _, _ = predictor.model.sam_mask_decoder(\n",
    "#     image_embeddings=predictor._features[\"image_embed\"][-1].unsqueeze(0).to(device),\n",
    "#     image_pe=predictor.model.sam_prompt_encoder.get_dense_pe(),\n",
    "#     sparse_prompt_embeddings=sparse_embeddings,\n",
    "#     dense_prompt_embeddings=dense_embeddings,\n",
    "#     multimask_output=True,\n",
    "#     repeat_image=False,\n",
    "#     high_res_features=[feat_level[-1].unsqueeze(0).to(device) for feat_level in predictor._features[\"high_res_feats\"]],\n",
    "# )\n",
    "# final_mask = predictor._transforms.postprocess_masks(prd_masks, predictor._orig_hw[-1])\n",
    "masks, scores, logits = predictor.predict(\n",
    "    point_coords=input_point,\n",
    "    point_labels=input_label,\n",
    "    multimask_output=False,\n",
    ")\n",
    "print(masks)\n",
    "\n",
    "# Save the mask\n",
    "# cv2.imwrite(mask_path, masks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e12cb1dd-69dc-412a-be59-467f668282d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask successfully saved to C:\\Users\\sinha\\segment-anything-2\\notebooks\\mask.png\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Assuming masks is a binary numpy array (0s and 1s) with an extra dimension\n",
    "# Remove the extra dimension\n",
    "if masks.ndim == 3 and masks.shape[0] == 1:\n",
    "    masks = masks.squeeze(0)  # Remove the first dimension\n",
    "\n",
    "# Convert binary mask to uint8 format (0 or 255)\n",
    "masks = (masks * 255).astype(np.uint8)\n",
    "\n",
    "# Define the path where the mask should be saved\n",
    "mask_output_path = r\"C:\\Users\\sinha\\segment-anything-2\\notebooks\\mask.png\"\n",
    "\n",
    "# Check if the directory exists\n",
    "output_dir = os.path.dirname(mask_output_path)\n",
    "if not os.path.exists(output_dir):\n",
    "    print(f\"Directory does not exist: {output_dir}\")\n",
    "else:\n",
    "    # Attempt to save the mask and check the result\n",
    "    success = cv2.imwrite(mask_output_path, masks)\n",
    "    if success:\n",
    "        print(f\"Mask successfully saved to {mask_output_path}\")\n",
    "    else:\n",
    "        print(f\"Failed to save mask to {mask_output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
